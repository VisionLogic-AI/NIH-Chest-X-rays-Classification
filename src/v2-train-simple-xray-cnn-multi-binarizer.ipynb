{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#conda create --name tf_gpu tensorflow-gpu\n",
    "#conda activate tf_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation Files:\n",
    "\n",
    "conda install -c anaconda keras-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes from the Readme File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The usage of the data set is unrestricted. But you should provide the link to our original download site, acknowledge the NIH Clinical Center and provide a citation to our CVPR 2017 paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: ‘No finding’ means the 14 listed disease patterns are not found in the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/nih-chest-xrays/data\n",
    "\n",
    "Citations\n",
    "Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf\n",
    "\n",
    "NIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community\n",
    "\n",
    "Original source files and documents: https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Behind Dataset for Final Writeup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset was gathered by the NIH and contains over 100,000 anonymized chest x-ray images from more than 30,000 patients.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "19ff9d0b-d32d-4325-a19f-2ccbfa7d2dff",
    "_uuid": "9206c5863724e1af01281071e0f0ee5109ef431b"
   },
   "source": [
    "# Goal\n",
    "The goal is to use a simple model to classify x-ray images in Keras, the notebook how to use the ```flow_from_dataframe``` to deal with messier datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f42f6560-edf0-4efb-85a6-6e945e50895b",
    "_uuid": "3300a1edbf2e8122d88093998eb503a6fab8a719"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df = pd.read_csv('./data/Data_Entry_2017.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join('.', 'data', 'images*', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "\n",
    "\n",
    "#all_xray_df['Patient Age'] = all_xray_df['Patient Age'].map(lambda x: int(x[:-1]))\n",
    "all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0d674a43-170f-4ca6-a9b4-edc64ab89c35",
    "_uuid": "9a1669750b7cffcae1b7a2eb4307b85472dff192"
   },
   "source": [
    "# Preprocessing Labels\n",
    "Here we take the labels and make them into a more clear format. The primary step is to see the distribution of findings and then to convert them to simple binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d75ed03e-986d-4f3f-81af-c910d194f390",
    "_uuid": "0c5123f65edf349f8ad2e5c5f55ac484d974f5c7"
   },
   "outputs": [],
   "source": [
    "label_counts = all_xray_df['Finding Labels'].value_counts()[:15]\n",
    "fig, ax1 = plt.subplots(1,1,figsize = (12, 8))\n",
    "ax1.bar(np.arange(len(label_counts))+0.5, label_counts)\n",
    "ax1.set_xticks(np.arange(len(label_counts))+0.5)\n",
    "_ = ax1.set_xticklabels(label_counts.index, rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a9ff3a04-254d-4667-b0a3-869bbd92b08b",
    "_uuid": "78b4425b0a8932bb33864b5a3712611004687a48"
   },
   "outputs": [],
   "source": [
    "all_xray_df['Finding Labels'] = all_xray_df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\n",
    "from itertools import chain\n",
    "all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "all_labels = [x for x in all_labels if len(x)>0]\n",
    "print('All Labels ({}): {}'.format(len(all_labels), all_labels))\n",
    "for c_label in all_labels:\n",
    "    if len(c_label)>1: # leave out empty labels\n",
    "        all_xray_df[c_label] = all_xray_df['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\n",
    "all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "05b6c7d9-4869-40b5-a70b-53957c69f021",
    "_uuid": "64653f9f57d3a5b952f1f1a709eb70ca38761a0d"
   },
   "source": [
    "### Clean categories\n",
    "Since we have too many categories, we can prune a few out by taking the ones with only a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dd273e26-63ab-4d76-925d-9c1c2c1ba69c",
    "_uuid": "9f368f9ea8947d8fc1dacf3988c58b6a2bf5fffc"
   },
   "outputs": [],
   "source": [
    "# keep at least 1000 cases\n",
    "MIN_CASES = 1000\n",
    "all_labels = [c_label for c_label in all_labels if all_xray_df[c_label].sum()>MIN_CASES]\n",
    "print('Clean Labels ({})'.format(len(all_labels)), \n",
    "      [(c_label,int(all_xray_df[c_label].sum())) for c_label in all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01478f44-5beb-4a6c-aa27-77db4b9abf10",
    "_uuid": "cf071cfa49f2886d9795cdf6067a8afdafd6f458"
   },
   "outputs": [],
   "source": [
    "# since the dataset is very unbiased, we can resample it to be a more reasonable collection\n",
    "# weight is 0.1 + number of findings\n",
    "sample_weights = all_xray_df['Finding Labels'].map(lambda x: len(x.split('|')) if len(x)>0 else 0).values + 4e-2\n",
    "sample_weights /= sample_weights.sum()\n",
    "all_xray_df = all_xray_df.sample(40000, weights=sample_weights)\n",
    "\n",
    "label_counts = all_xray_df['Finding Labels'].value_counts()[:15]\n",
    "fig, ax1 = plt.subplots(1,1,figsize = (12, 8))\n",
    "ax1.bar(np.arange(len(label_counts))+0.5, label_counts)\n",
    "ax1.set_xticks(np.arange(len(label_counts))+0.5)\n",
    "_ = ax1.set_xticklabels(label_counts.index, rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "277caf38-f13d-4939-9eef-55efaf73020e",
    "_uuid": "cb9f18a64740580e1c541b8631d45720be5f36fe"
   },
   "outputs": [],
   "source": [
    "label_counts = 100*np.mean(all_xray_df[all_labels].values,0)\n",
    "fig, ax1 = plt.subplots(1,1,figsize = (12, 8))\n",
    "ax1.bar(np.arange(len(label_counts))+0.5, label_counts)\n",
    "ax1.set_xticks(np.arange(len(label_counts))+0.5)\n",
    "ax1.set_xticklabels(all_labels, rotation = 90)\n",
    "ax1.set_title('Adjusted Frequency of Diseases in Patient Group')\n",
    "_ = ax1.set_ylabel('Frequency (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a1367d63-4c7b-4e47-b19f-9a26d1f89476",
    "_uuid": "6ed6489bbd618fb419ceca2bbd8c300694f1d4ed",
    "scrolled": true
   },
   "source": [
    "# Prepare Training Data\n",
    "Here we split the data into training and validation sets and create a single vector (disease_vec) with the 0/1 outputs for the disease status (what the model will try and predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "185ae07c-17a2-46d3-81ca-4007e88cbf86",
    "_uuid": "6f82e5ae33e1c0fcfa33ac51a26885cb5a0e6bb1"
   },
   "outputs": [],
   "source": [
    "all_xray_df['disease_vec'] = all_xray_df.apply(lambda x: [x[all_labels].values], 1).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5feddc96-7958-4d34-81f1-10eba94ab1b6",
    "_uuid": "8c7dc722de08243cc763d23928708f9c040bbdc6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(all_xray_df, \n",
    "                                   test_size = 0.25, \n",
    "                                   random_state = 2018,\n",
    "                                   stratify = all_xray_df['Finding Labels'].map(lambda x: x[:4]))\n",
    "print('train', train_df.shape[0], 'validation', valid_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7178cb8d-f220-4422-93ae-2469f0c97493",
    "_uuid": "0115346eb989e7eaeffa3643d05c654ec5ceb7c2"
   },
   "source": [
    "# Create Data Generators\n",
    "Here we make the data generators for loading and randomly transforming images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_df['newLabel'] = valid_df.apply(lambda x: x['Finding Labels'].split('|'), axis=1)\n",
    "train_df['newLabel'] = train_df.apply(lambda x: x['Finding Labels'].split('|'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "30eaf01b-8ec0-4407-9d25-f87ca1f48e8a",
    "_uuid": "b5e42124376584390e925a06c4cee564285e43b3"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "IMG_SIZE = (128, 128)\n",
    "core_idg = ImageDataGenerator(samplewise_center=True, \n",
    "                              samplewise_std_normalization=True, \n",
    "                              horizontal_flip = True, \n",
    "                              vertical_flip = False, \n",
    "                              height_shift_range= 0.05, \n",
    "                              width_shift_range=0.1, \n",
    "                              rotation_range=5, \n",
    "                              shear_range = 0.1,\n",
    "                              fill_mode = 'reflect',\n",
    "                              zoom_range=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = core_idg.flow_from_dataframe(dataframe=train_df, \n",
    "                             directory=None,\n",
    "                             x_col = 'path',\n",
    "                            y_col = 'newLabel', \n",
    "                             class_mode = 'categorical',\n",
    "                            classes = all_labels,\n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'grayscale',\n",
    "                            batch_size = 32)\n",
    "\n",
    "valid_gen = core_idg.flow_from_dataframe(dataframe=valid_df, \n",
    "                             directory=None,\n",
    "                             x_col = 'path',\n",
    "                            y_col = 'newLabel', \n",
    "                             class_mode = 'categorical',\n",
    "                            classes = all_labels,\n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'grayscale',\n",
    "                            batch_size = 256) # we can use much larger batches for evaluation\n",
    "\n",
    "train_X, train_Y = next(core_idg.flow_from_dataframe(dataframe=train_df, \n",
    "                             directory=None,\n",
    "                             x_col = 'path',\n",
    "                            y_col = 'newLabel', \n",
    "                             class_mode = 'categorical',\n",
    "                            classes = all_labels,\n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'grayscale',\n",
    "                            batch_size = 1024))\n",
    "\n",
    "test_X, test_Y = next(core_idg.flow_from_dataframe(dataframe=valid_df, \n",
    "                             directory=None,\n",
    "                             x_col = 'path',\n",
    "                            y_col = 'newLabel', \n",
    "                             class_mode = 'categorical',\n",
    "                            classes = all_labels,\n",
    "                            target_size = IMG_SIZE,\n",
    "                             color_mode = 'grayscale',\n",
    "                            batch_size = 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0621048f-3d4c-4eb7-aaee-0c53f7cbf26a",
    "_uuid": "2852ef2fe07eb6d8e2caf056d1428d153afad65f"
   },
   "outputs": [],
   "source": [
    "t_x, t_y = next(train_gen)\n",
    "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -1.5, vmax = 1.5)\n",
    "    c_ax.set_title(', '.join([n_class for n_class, n_score in zip(all_labels, c_y) \n",
    "                             if n_score>0.5]))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cce3c89c-4292-4092-9317-ca066d72e354",
    "_uuid": "5b0ec78f2ffa8137dce37744f38a40782aafb54d"
   },
   "source": [
    "# Create a simple model\n",
    "Here we make a simple model to train using MobileNet as a base and then adding a GAP layer (Flatten could also be added), dropout, and a fully-connected layer to calculate specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "235846d4-3ef1-4888-a276-6134ad572415",
    "_uuid": "a447f8cc7274c06555849a40881fb903aca7001a"
   },
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, callbacks, regularizers\n",
    "base_mobilenet_model = MobileNet(input_shape =  t_x.shape[1:], \n",
    "                                 include_top = False, weights = None)\n",
    "multi_disease_model = Sequential()\n",
    "multi_disease_model.add(base_mobilenet_model)\n",
    "multi_disease_model.add(GlobalAveragePooling2D())\n",
    "multi_disease_model.add(Dropout(0.5))\n",
    "multi_disease_model.add(Dense(512))\n",
    "multi_disease_model.add(Dropout(0.5))\n",
    "multi_disease_model.add(Dense(len(all_labels), activation = 'sigmoid'))\n",
    "multi_disease_model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
    "                           metrics = ['binary_accuracy', 'mae'])\n",
    "multi_disease_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1acd1753-dae2-440d-a9d5-f3bbebfe446f",
    "_uuid": "90a131f99bea8b77fba54024e261a97e24dfc6c1"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('xray_class')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=5)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e5ad629e-15a3-4c96-ac74-db8077b101fe",
    "_uuid": "eba5247e4a767def4b4ead6e2f02f3c334edbbf0"
   },
   "source": [
    "# First Round\n",
    "Here we do a first round of training to get a few initial low hanging fruit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c269083c-7617-4c16-8f96-353c1f3d8eef",
    "_uuid": "cbdfcf827510dfb3bdba6ab08ff60e21838d6987"
   },
   "outputs": [],
   "source": [
    "multi_disease_model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch=1000,\n",
    "                                  validation_data = (test_X, test_Y), \n",
    "                                  epochs = 1, \n",
    "                                  callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "571df602-c0f1-4ba0-9f1e-0c9fdc10eff3",
    "_uuid": "392f3973459dcbb3fc998191baca4d047995d351"
   },
   "source": [
    "# Check Output\n",
    "Here we see how many positive examples we have of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d9f519ef-4b4f-4bd9-8989-643b21cb8904",
    "_uuid": "308a85cd454ad05e451ec1db99432c6fe85e8e41"
   },
   "outputs": [],
   "source": [
    "for c_label, s_count in zip(all_labels, 100*np.mean(test_Y,0)):\n",
    "    print('%s: %2.2f%%' % (c_label, s_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "73a8dba1-3d8e-47ec-b945-3101ad57dcef",
    "_uuid": "0878962d872569c57b0f959751c522ae928f92e3"
   },
   "outputs": [],
   "source": [
    "pred_Y = multi_disease_model.predict(test_X, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7a0fc649-158c-4844-9b05-f6a62214009f",
    "_uuid": "4acd4919e0ba24e2d06ef75fbb054a907a582c7a"
   },
   "source": [
    "# ROC Curves\n",
    "While a very oversimplified metric, we can show the ROC curve for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc076f02-0bf2-4d3f-a6ab-f192bed9e6a9",
    "_uuid": "d7168d90928d842c295c64e2446e84eb068fd391"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
    "for (idx, c_label) in enumerate(all_labels):\n",
    "    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n",
    "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "fig.savefig('barely_trained_net.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers_list = [('sgd',optimizers.SGD( lr=.1) ),\n",
    "                   ('sgd_momentum',optimizers.SGD(lr=.1, momentum=.9) ),\n",
    "                   ('adagrad',optimizers.Adagrad()),\n",
    "                   ('adadelta',optimizers.Adadelta()),\n",
    "                   ('adam', optimizers.Adam()) \n",
    "                  ]\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=5)\n",
    "\n",
    "\n",
    "\n",
    "callbacks_list = [early, ModelCheckpoint( filepath=WEIGHTS_PATH, monitor='val_loss', mode='min', save_best_only=True )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "for optimizer in optimizers_list:\n",
    "    multi_disease_model.compile(optimizer = optimizer[1], loss = 'binary_crossentropy',\n",
    "                           metrics = ['binary_accuracy', 'mae'])\n",
    "    \n",
    "    history = multi_disease_model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch=1000,\n",
    "                                  validation_data = (test_X, test_Y), \n",
    "                                  epochs = 50, \n",
    "                                  callbacks = callbacks_list)\n",
    "      \n",
    "    \n",
    "    plt.plot(history.history['val_loss'])\n",
    "    \n",
    "plt.legend([x[0] for x in optimizers_list], loc='upper right')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "plt.savefig('optimizer_selection.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it appears that Adagrad, and adadelt may reach convergence faster, there is no substantially different loss as a result of optimizer selection.  When this function was run with larger numbers of training examples per epoch, adam outperformed (graphic not shown).  Based on the results shown in the figure above, we can accept the use of adam based on this particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Batch Size and the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Accumulation Borrowed from following location:\n",
    "# https://stackoverflow.com/questions/55268762/how-to-accumulate-gradients-for-large-batch-sizes-in-keras\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.legacy import interfaces\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "class AdamAccumulate(Optimizer):\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False, accum_iters=1, **kwargs):\n",
    "        if accum_iters < 1:\n",
    "            raise ValueError('accum_iters must be >= 1')\n",
    "        super(AdamAccumulate, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "        self.accum_iters = K.variable(accum_iters, K.dtype(self.iterations))\n",
    "        self.accum_iters_float = K.cast(self.accum_iters, K.floatx())\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "\n",
    "        completed_updates = K.cast(K.tf.floordiv(self.iterations, self.accum_iters), K.floatx())\n",
    "\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * completed_updates))\n",
    "\n",
    "        t = completed_updates + 1\n",
    "\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        # self.iterations incremented after processing a batch\n",
    "        # batch:              1 2 3 4 5 6 7 8 9\n",
    "        # self.iterations:    0 1 2 3 4 5 6 7 8\n",
    "        # update_switch = 1:        x       x    (if accum_iters=4)  \n",
    "        update_switch = K.equal((self.iterations + 1) % self.accum_iters, 0)\n",
    "        update_switch = K.cast(update_switch, K.floatx())\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        gs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat, tg in zip(params, grads, ms, vs, vhats, gs):\n",
    "\n",
    "            sum_grad = tg + g\n",
    "            avg_grad = sum_grad / self.accum_iters_float\n",
    "\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * avg_grad\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(avg_grad)\n",
    "\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, (1 - update_switch) * vhat + update_switch * vhat_t))\n",
    "            else:\n",
    "                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, (1 - update_switch) * m + update_switch * m_t))\n",
    "            self.updates.append(K.update(v, (1 - update_switch) * v + update_switch * v_t))\n",
    "            self.updates.append(K.update(tg, (1 - update_switch) * sum_grad))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, (1 - update_switch) * p + update_switch * new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(AdamAccumulate, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was determined earlier in the training data loader\n",
    "initialBS = 32\n",
    "STEPS_PER_EPOCH = 1000\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeModel():\n",
    "    model = Sequential()\n",
    "    model.add(base_mobilenet_model)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(all_labels), activation = 'sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we set the steps per epoch and the batch size so regardless of learning rate, we are looking at the same number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "run_this_code = True # This Code Block Takes a long time to run\n",
    "if run_this_code:\n",
    "    train_results = defaultdict(dict)\n",
    "    test_results  = defaultdict(dict)\n",
    "\n",
    "    for batch in [16, 8, 4, 2, 1]: #Number of batches before gradient accumulation\n",
    "        #for lr in [1e-4, 5e-4, 2e-4, 1e-3, 2e-3, 5e-3, 1e-2, 2e-2, 1e-1]:\n",
    "        for lr in [1e-4, 5e-4]:\n",
    "            print ('Running Batch size : ', batch*initialBS, 'Learning Rate : ', lr)\n",
    "            predictions_train = pd.DataFrame()\n",
    "            predictions_test  = pd.DataFrame()\n",
    "            \n",
    "            opt = AdamAccumulate(lr=lr, accum_iters=batch) #Set the gradient accumulation value\n",
    "            multi_disease_model = MakeModel()\n",
    "            multi_disease_model.compile(optimizer = opt, loss = 'binary_crossentropy',\n",
    "                   metrics = ['binary_accuracy', 'mae'])\n",
    "\n",
    "            history = multi_disease_model.fit_generator(train_gen, \n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_data = (test_X, test_Y), \n",
    "                          epochs = EPOCHS, \n",
    "                          callbacks = callbacks_list)\n",
    "\n",
    "            multi_disease_model.load_weights(weight_path)\n",
    "\n",
    "            p = history.history['val_loss'][0]\n",
    "\n",
    "            del multi_disease_model, history\n",
    "            gc.collect()\n",
    "            print('*'*50)\n",
    "            print('')\n",
    "\n",
    "            test_results[batch][lr]  = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mohamed's Comments\n",
    "\n",
    "    A learning rate 0.01 seems to be the best option when combined with a batchsize of 512, 1024, 2048\n",
    "    Some published papers suggest that for sgd based optimizers, using small batch sizes leads to a better generalization (https://openreview.net/pdf?id=BJij4yg0Z , https://openreview.net/forum?id=H1oyRlYgg)\n",
    "    We should maximize the ratio LR / BS in order to avoid sharp minima in the los function (https://www.research.ed.ac.uk/portal/files/75846467/width_of_minima_reached_by_stochastic_gradient_descent_is_influenced_by_learning_rate_to_batch_size_ratio.pdf)\n",
    "\n",
    "So we should probably use LR = 0.01 and BS=1024. But I will use a BS of 2048 instead during the prototyping phase (NN will run faster so we can test more things). Later, we can switch back to LR=1024\n",
    "\n",
    "    When prototyping, choose the largest BS without sacrificing too much the performance\n",
    "    I have found in practice that the optimal batch size is independent of NN architecture, so always tune the batch size first, then take the highest learning rate possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now We Try Different Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Image Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c2fe734-8f87-4537-81e0-d0ce68bc2545",
    "_uuid": "8a03574bc1f2b3441538c2408fc158324d6d23df",
    "collapsed": true
   },
   "source": [
    "# Continued Training\n",
    "Now we do a much longer training process to see how the results improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "80b6ede7-052b-4fa2-ab97-bed0199db681",
    "_uuid": "0b35cd7ef77680d2aa180812622d89388b8fff63"
   },
   "outputs": [],
   "source": [
    "multi_disease_model.fit_generator(train_gen, \n",
    "                                  steps_per_epoch = 100,\n",
    "                                  validation_data =  (test_X, test_Y), \n",
    "                                  epochs = 100, \n",
    "                                  callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best weights\n",
    "multi_disease_model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97dd6d26-8d12-4aa5-8544-6bde2f8200a6",
    "_uuid": "2f358c745432e497adfac31c6d97abc8b61bd0eb"
   },
   "outputs": [],
   "source": [
    "pred_Y = multi_disease_model.predict(test_X, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e754b40-8492-4f5b-a19f-a0bfd2c9b4f4",
    "_uuid": "db77f749926ac5936e860617b22713715a69157a"
   },
   "outputs": [],
   "source": [
    "# look at how often the algorithm predicts certain diagnoses \n",
    "for c_label, p_count, t_count in zip(all_labels, \n",
    "                                     100*np.mean(pred_Y,0), \n",
    "                                     100*np.mean(test_Y,0)):\n",
    "    print('%s: Dx: %2.2f%%, PDx: %2.2f%%' % (c_label, t_count, p_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "31990e46-1437-4945-aaa5-b31b377d6538",
    "_uuid": "a5dd26025391067cd0220b7502b193e5e095edbd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
    "for (idx, c_label) in enumerate(all_labels):\n",
    "    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n",
    "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "fig.savefig('trained_net.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a50931b1-4d88-4b2f-93f7-ad3c9de31e5c",
    "_uuid": "8ad061d6b08b9ff383f682c35c8b55f30fb560a1"
   },
   "source": [
    "# Show a few images and associated predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "714ada59-850d-4d88-b983-2f9107f36e70",
    "_uuid": "e5191706a55a5ab8b68773d0b881aac4b020795a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sickest_idx = np.argsort(np.sum(test_Y, 1)<1)\n",
    "fig, m_axs = plt.subplots(4, 2, figsize = (16, 32))\n",
    "for (idx, c_ax) in zip(sickest_idx, m_axs.flatten()):\n",
    "    c_ax.imshow(test_X[idx, :,:,0], cmap = 'bone')\n",
    "    stat_str = [n_class[:6] for n_class, n_score in zip(all_labels, \n",
    "                                                                  test_Y[idx]) \n",
    "                             if n_score>0.5]\n",
    "    pred_str = ['%s:%2.0f%%' % (n_class[:4], p_score*100)  for n_class, n_score, p_score in zip(all_labels, \n",
    "                                                                  test_Y[idx], pred_Y[idx]) \n",
    "                             if (n_score>0.5) or (p_score>0.5)]\n",
    "    c_ax.set_title('Dx: '+', '.join(stat_str)+'\\nPDx: '+', '.join(pred_str))\n",
    "    c_ax.axis('off')\n",
    "fig.savefig('trained_img_predictions.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "48aff5bd-c927-44ae-abd3-1de884969d3b",
    "_uuid": "817ed988ed86bc7941065eeac676d1b50cef5d0b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68354276-ebbb-46ca-acd6-9e8cd1354bba",
    "_uuid": "6c186e1070db679fb62a4a08e4f42a7ece925aff"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
